{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_a8JbQII6Uz"
      },
      "source": [
        "# Final Project: Yelp and Food Safety\n",
        "#### Exploring the San Francisco Restaurant World\n",
        "\n",
        "In this project, we will investigate a subset of the restaurants and related information from them adapted from Yelp data located in San Francisco, California. You will first explore some of the data about the restaurants themselves, calculating some summary statistics and trying to figure out some patterns in the data. Next we will merge that with a list of health inspection scores and violations that have been [made available by the San Francisco Department of Public Health](https://data.sfgov.org/Health-and-Social-Services/Restaurant-Scores-LIVES-Standard/pyih-qa8i). Lastly, we will run some linear regression analysis to see if there is any meaningful relationships in the data. \n",
        "\n",
        "Please remember to not delete any cells you didn't create and only edit code in places marked with `##Your Code Here...` or where there are ellipses like `...`\n",
        "\n",
        "If you have any questions or get stuck or anything, feel free to come to our project party on Wednesday August 8th from 5pm to 8pm PST or schedule office hours!\n",
        "\n",
        "**Helpful Resource:**\n",
        "\n",
        "* [Python Reference](https://docs.google.com/document/d/1zpTTl47NoGf2A3_oE1YusLyb-cF2sZMALdCMM5dpYIA/edit): Cheat sheet for Python and other functions used in this course\n",
        "\n",
        "\n",
        "To get started on the final project, first run the following cell to import some necessary packages as well as to download the data, and have fun! \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNkNPtkKI6U3"
      },
      "outputs": [],
      "source": [
        "# importing some helful libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# downloading necessary data and helper functions\n",
        "!wget https://raw.githubusercontent.com/dnnyw/intro-to-ds-resources/main/final_project/data/businesses.csv\n",
        "!wget https://raw.githubusercontent.com/dnnyw/intro-to-ds-resources/main/final_project/data/inspections.csv\n",
        "!wget https://raw.githubusercontent.com/dnnyw/intro-to-ds-resources/main/final_project/project_helper.py\n",
        "!wget https://raw.githubusercontent.com/dnnyw/intro-to-ds-resources/main/final_project/functions.py\n",
        "from project_helper import * "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwRGKXhRI6U4"
      },
      "source": [
        "# **1. San Francisco Restaurant Data**\n",
        "\n",
        "In this section you'll be learning a few extra useful features of _dataframes_, which we previously used in lab 3 as a way to managing data for analysis. \n",
        "\n",
        "As you might have noticed, the package we are using is called _Pandas_, which is the most commonly used package to clean and analyze data. You will learn some of the most important features of manipulating data using Pandas, and get a feel for exploring data using Python. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkyYO05dI6U5"
      },
      "source": [
        "## Part One: Loading the Data\n",
        "\n",
        "As mentioned in lecture, we can use Pandas to read many differe types of data format and read it into a table. The most common are `.csv` files, which stand for comma-separated-values. \n",
        "\n",
        "We've downloaded the two `.csv` files for you above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ISm9SKWI6U5"
      },
      "source": [
        "As a side note, when you reopen this project in Google Colab, your code will remain, however it will delete any files you downloaded in the previous session. Just a friendly reminder to rerun that cell block to download the files each time you restart Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvxoD4w5I6U6"
      },
      "source": [
        "\n",
        "### Question 1:\n",
        "\n",
        "Now, load the files, named `businesses.csv` and `inspections.csv` into Pandas dataframes named `bus`, and `ins` respectively. \n",
        "\n",
        "Run the cell afterwards to check if you did this correctly.\n",
        "\n",
        "_hint: you can use `pd.read_csv` to do this_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hQ-GDfzI6U6"
      },
      "outputs": [],
      "source": [
        "## Your Code Here...\n",
        "bus = ...\n",
        "ins = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSYHVyfEI6U7"
      },
      "outputs": [],
      "source": [
        "check('q1a', [bus, ins])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPCsW6wAI6U8"
      },
      "source": [
        "Now that you've read in the files, let's try some `pd.DataFrame` methods ([docs](https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.html)).\n",
        "We can use the `DataFrame.head` method to show the top few lines of the `bus` and `ins` dataframes. To show multiple return outputs in one single cell, you can useÂ `display()`.\n",
        "\n",
        "Run the following cell to display the both data frames. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V93vhUFII6U8"
      },
      "outputs": [],
      "source": [
        "display(bus.head(), ins.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCY6xn3nI6U9"
      },
      "source": [
        "You can also use the `DataFrame.describe()` method to learn about the numeric columns of each dataframe. It can be handy for computing summaries of various statistics of our dataframes. \n",
        "\n",
        "Try it out with our two dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZP7uwf5I6U9"
      },
      "outputs": [],
      "source": [
        "# Try displaying the DataFrame.describe outputs for bus and ins\n",
        "\n",
        "## Your code here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYF8mHahI6U-"
      },
      "source": [
        "From its name alone, we expect the `bus.csv` file to contain information about the restaurants. Let's do some Exploratory Data Analysis (EDA), and see if we can get a better understanding of the data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSVsHL1qI6U-"
      },
      "source": [
        "## Part 2: Exploring the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiTAQlhzI6U-"
      },
      "source": [
        "In lab 3, we refered to the data in a column as an array. Another term that it can be called is a `Series`, which is just a fancier version of an array. \n",
        "\n",
        "The nice thing about Series' is that they have lots of [built in functions within them](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) that are called methods. \n",
        "\n",
        "\n",
        "- The [`Series.unique`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) method returns an array of all the unique entries inside of a `Series`. \n",
        "\n",
        "- The [`Series.value_counts`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) method returns a new series that lists the number of occurencies of each unique element in a `Series`. The list is returned in descending order.\n",
        "\n",
        "Read the documentation is you want a deeper look at these functions, you can also look at some examples of how they are used. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SE8s4yrI6U_"
      },
      "source": [
        "### Question 2a:\n",
        "\n",
        "Notice that there are two different identifiers in two different columns for businesses in our dataframe, the columnd `bid` which contains identification numbers, as well as the column `name` which contains the name of each business. \n",
        "\n",
        "For both of these variables, figure out the number of unique entires, and assign that  to `n_bus`, and `n_bid`.\n",
        "\n",
        "_hint: you can use the python built in function `len()` to calculate the number of items in the array_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoJTZ9a9I6U_"
      },
      "outputs": [],
      "source": [
        "## Your Code Here...\n",
        "n_bus = ...\n",
        "n_bid = ...\n",
        "\n",
        "\n",
        "# this line is to print your results\n",
        "print(' Number of Unique Businesses: ', n_bus, \"\\n Number of Unique Business ID: \", n_bid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP51WuX2I6U_"
      },
      "outputs": [],
      "source": [
        "check('q2a', [n_bus, n_bid])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npRiRdxKI6VA"
      },
      "source": [
        "Interesting. There are more unique `bid`'s than there are `name`'s. As you might have guessed, this is because there might be more than one location of a restaurant, yet both the `bid` and `name` can be used to identify restaurants. Since `bid` also distinguishes between the locations of a restaurant, we say `bid` is more _granular_ in data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho11LaLhI6VA"
      },
      "source": [
        "### Question 2b:\n",
        "\n",
        "Find the `name` of the restaurant with the most number of occurences in our dataset, and assign it's name as a string to `most_locations`. You can just type the string out to assign the variable. Use as many cells and lines as you need to find the restaurant, just remember to assign it to `most_locations` in the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap4STys-I6VA"
      },
      "outputs": [],
      "source": [
        "## Your Answer Here...\n",
        "...\n",
        "most_locations = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okSyYG5NI6VB"
      },
      "outputs": [],
      "source": [
        "check('q2b', most_locations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb862fhdI6VB"
      },
      "source": [
        "### Question 2c:\n",
        "\n",
        "The cool thing about Series is that when you apply comparison operators to them, it does it for each entry in the Series. Figure out how many restaurants chains have more than one location, and assign that to `num_mult_locations`. \n",
        "\n",
        "_Hint: Remember that True and 1 and the same. First try getting a series of booleans using comparison operators and then use that with the built in function `sum` to find the number of locations. When you add `True` values together, it is the same as adding a bunch of ones._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MeuqltMI6VB"
      },
      "outputs": [],
      "source": [
        "## Your Code Here... \n",
        "num_mult_locations = ...\n",
        "num_mult_locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joJc0DJXI6VB"
      },
      "outputs": [],
      "source": [
        "check('q2c', num_mult_locations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J66BzMGNI6VC"
      },
      "source": [
        "You can also use other comparisons to return a series of booleans, refer to Lecture 2 for a list of comparison operators. This is very useful for filtering data from dataframes, which we shall do in the next problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beawqWU6I6VC"
      },
      "source": [
        "## Part 3: Exploring the Data (cont.)\n",
        "\n",
        "So far, you've had a chance to select and analyze data from a single column of a dataframe. This is useful when we want to analyze information accross the observations we have (for example, accross all restaurants in our dataset). Often times we also want only consider a certain subset of our observations (for example only selecting the Italian restaurants). \n",
        "\n",
        "There are [many ways to select subsets of data](https://pandas.pydata.org/docs/user_guide/indexing.html), but we will focus on boolean-indexing. \n",
        "\n",
        "\n",
        "Let's walk through a short little example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpHRStDtI6VC"
      },
      "outputs": [],
      "source": [
        "# Output data frame for convenience\n",
        "bus.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQL0zMhgI6VC"
      },
      "source": [
        "Say I really liked _Burma Superstar_ and want all other restaurants with `type == 'Burmese'`. \n",
        "\n",
        "First, I can extract the `type` column as a Series, similarly to how we have done in Lab 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQWV7CTKI6VC"
      },
      "outputs": [],
      "source": [
        "# Just run this cell\n",
        "types = bus['type']\n",
        "types\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-4n8xKiI6VC"
      },
      "source": [
        "You'll notice that on the left of the Series output, there are numbers that each correspond to a specific level of price. This is called the index, and it corresponds to the index (also on the left) in the `bus` dataframe. \n",
        "\n",
        "_Indices don't have to be in ascending order, and they also do not have to be numbers either, but more on this later._\n",
        "\n",
        "Next, like in part 2c, I can use a comparison operator to find all indices that are equal to `'Burmese'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGD5rkQJI6VD"
      },
      "outputs": [],
      "source": [
        "burmese = types == 'Burmese'\n",
        "burmese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AYPOURTI6VD"
      },
      "source": [
        "_Burma Superstar_, in index position 1 returns true, as expected. We can now use this Series to index into the `bus` dataframe. Rows that correspond `True` indices in the \"indexer\" will be kept, and all falses will be dropped. This will not change the original `bus` dataframe, so we have to reassign it to a new variable if we want to keep using it. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HCv8n9HI6VD"
      },
      "outputs": [],
      "source": [
        "burmese_restaurants = bus[burmese]\n",
        "burmese_restaurants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrgzX1njI6VD"
      },
      "source": [
        "We walked through it step by step, but this can be done in one line as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmytn1SFI6VD"
      },
      "outputs": [],
      "source": [
        "burmese_restaurants = bus[bus['type'] == 'Burmese']\n",
        "burmese_restaurants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4joycfd1I6VD"
      },
      "source": [
        "### Question 3a:\n",
        "\n",
        "Using boolean-indexing, create a new dataframe that only contains the rows in `bus` for the restaurant you found in part 2b (the string you assigned to `most_locations`) and assign it to `most_locations_df`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxZlhozXI6VE"
      },
      "outputs": [],
      "source": [
        "## Your Code Here...\n",
        "most_locations_df = ...\n",
        "most_locations_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrRgGt6RI6VE"
      },
      "outputs": [],
      "source": [
        "check('q3a', most_locations_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g49uMqVeI6VE"
      },
      "source": [
        "We can also do more complicated selects over multiple different columns. As we've mentioned, the syntax in Python is very similar to English. \n",
        "\n",
        "Say I wanted to find restaurants that were both `type == 'Chinese'` AND had `price == '$$'`. The syntax would be exactly that! \n",
        "\n",
        "One finicky note however is that you cannot use `and` or `or`, instead you use the ampersand `&` and the pipe symbol `|` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXqkAI-cI6VE"
      },
      "outputs": [],
      "source": [
        "# run this cell and see what it does\n",
        "chinese_2 = bus[(bus['type'] == 'Chinese') & (bus['price'] == '$$')]\n",
        "chinese_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uUMTBFiI6VE"
      },
      "source": [
        "All of these restaurants are _both_ Chinese cuisine and have a 2-dollar sign rating. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRNKcy-hI6VE"
      },
      "source": [
        "### Question 3b: \n",
        "\n",
        "Create a dataframe that contains all restaurants that have less than or equal to a 4.0 rating and strictly more than 1,000 reviews. Then, using this subset, figure out how many restaurants are each of the four price categoreies ('\\$', '\\$\\$', '\\$\\$\\$', and '\\$\\$\\$\\$') and assign it as a Series to the variable `q3b`. The series should be ordered from largest to smallest in terms of count.\n",
        "\n",
        "Your answer should have the indices be the corresponding price categories, and the counts of each categories as the values for each index. \n",
        "\n",
        "_Hint: use `series.value_counts()`_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8er5fL5I6VF"
      },
      "outputs": [],
      "source": [
        "## Your Code Here...\n",
        "q3b = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw0rcEP8I6VF"
      },
      "outputs": [],
      "source": [
        "check('q3b', q3b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zXwdrRSI6VF"
      },
      "source": [
        "### Question 3c:\n",
        "\n",
        "You might have noticed that some of the longitude and latitude data is -9999. This is typically a way to indicate that the data is missing when dealing with numerical data instead of just leaving the space blank. \n",
        "\n",
        "#### Part 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZ8XQTxI6VF"
      },
      "source": [
        "Filter out the data that has missing coordinate data from `bus` and assign it to the dataframe `bus_coords`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra8MUfV4I6VF"
      },
      "outputs": [],
      "source": [
        "## Your Code Here...\n",
        "bus_coords = ...\n",
        "bus_coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j62ehggI6VF"
      },
      "outputs": [],
      "source": [
        "check('q3c1', bus_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6ymHRhNI6VF"
      },
      "source": [
        "#### Part 2\n",
        "\n",
        "Next, we'll use a new package called Seaborn to plot the coordinates on a graph. The cool thing about Seaborn is that it allows for easy way to encode new information to aspects of the plot, like color! \n",
        "\n",
        "We've imported Seaborn for you, and made a basic plot of all the restaurants using the `scatterplot` function, and coded their `review_count` into the color of each point. Take a look at the [documentation](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) and have some fun plotting data from the `bus_location` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyPzmwrXI6VG"
      },
      "outputs": [],
      "source": [
        "# A little example\n",
        "import seaborn as sns\n",
        "\n",
        "sns.scatterplot(data = bus_coords, \n",
        "                x = 'latitude', \n",
        "                y = 'longitude', \n",
        "                hue = 'review_count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SScr_VY3I6VG"
      },
      "source": [
        "Some potential ideas:\n",
        "- Look at the distributions of some select cuisines, are they clustered around each other? (Probably subset the data before plotting)\n",
        "- Plot the locations of highly rated restaurants, and encode the size of each data point to the correspoding price rating \n",
        "- Encode the rating to the color of the data, and see if things are clustered together! \n",
        "\n",
        "\n",
        "Feel free to implement one of the ideas above, or try something new. \n",
        "\n",
        "Create your graph in the following code cell, and write down your findings as a comment in the same cell! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10uL0ACbI6VG"
      },
      "outputs": [],
      "source": [
        "## Your Code Here\n",
        "\n",
        "\n",
        "\n",
        "## Write down your discoveries as a comment! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLD8-j1MI6VG"
      },
      "source": [
        "# **2. Health Inspection Data**\n",
        "\n",
        "In this next section, we're going to merge the health inspection data with our business data. We will be doing some more statistics in this part in addition to exploring the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9l3Hg3bI6VG"
      },
      "outputs": [],
      "source": [
        "# run this cell\n",
        "ins.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iffrs9TxI6VH"
      },
      "source": [
        "Let's examine the inspection scores `ins['score']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDPvVaejI6VH"
      },
      "outputs": [],
      "source": [
        "# run this cell\n",
        "ins['score'].value_counts().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZJP9iaI6VH"
      },
      "source": [
        "It looks like there are a lot of inspections with the `'score'` of `-1`. In fact, only health inspections of the 'Routine - Unscheduled' type are scored. \n",
        "\n",
        "In the following cell, we used the [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function from pandas to join the inspection data with the business data. You don't know to know how it works, but if you're curious read more in the documentation. The merged datafram is called `ins_named`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bT5m3QnI6VH"
      },
      "outputs": [],
      "source": [
        "# run this cell\n",
        "ins_named = ins.merge(right = bus, how = 'inner', on = 'bid')\n",
        "ins_named.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AywG4S-bI6VI"
      },
      "source": [
        "### Question 4a:\n",
        "Select only the inspections that have a health score rating from the dataframe `ins_named`, and assign this resulting dataframe to the variable, `scores`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at9-F20SI6VI"
      },
      "outputs": [],
      "source": [
        "## You code here...\n",
        "scores = ...\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEgVxB8dI6VI"
      },
      "outputs": [],
      "source": [
        "check('q4a', scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39z1FvR9I6VI"
      },
      "source": [
        "### Question 4b: \n",
        "\n",
        "#### Part 1:\n",
        "Next, plot a bar chart of distribution of scores. There should be a bar for each of the discrete scores (a histogram would mask the details of the distribution)\n",
        "\n",
        "_Hint: You can use_ `series.value_counts()` _to get the heights, and then use_ `series.value_counts().index` _to get the index of the series for the categories in a bar chart_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CviPCQD7I6VI"
      },
      "outputs": [],
      "source": [
        "## Your Code Here...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6db2kMLI6VI"
      },
      "source": [
        "#### Part 2:\n",
        "\n",
        "Describe the qualities of the distribution of the inspection scores based on your histogram. Consider the skewness, the mean, the median, or any anomalous values. Are they any unusual features about this distribution? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSzx0Lj2I6VJ"
      },
      "source": [
        "_Write your answer in this cell:_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzcZngFRI6VJ"
      },
      "source": [
        "### Question 4c: \n",
        "\n",
        "Let's figure out which restaurant had the worst score in our sample of data. Use `scores` to find the lowest score.\n",
        "\n",
        "A method that might be useful is [`DataFrame.sort_values`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) (click on the link for the documentation). \n",
        "\n",
        "Then assign the name of the worst restaurant to `worst_restaurant`. You can just type the string into the variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rWJmOIXI6VJ"
      },
      "outputs": [],
      "source": [
        "## Your Code Here...\n",
        "\n",
        "worst_restaurant = ...\n",
        "worst_restaurant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7NVAvfhI6VJ"
      },
      "outputs": [],
      "source": [
        "check('q4c', worst_restaurant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1XmXbu7I6VJ"
      },
      "source": [
        "### Question 4d: \n",
        "\n",
        "Let's see which restaurant has had the most extreme improvement in its health inspection rating, aka scores. Let the \"swing\" of a restaurant be defined as the difference between its highest-ever and lowest-ever health inspection score. \n",
        "\n",
        "*Note*: The \"swing\" is of a specific business. There might be some restaurants with multiple locations; each location has its own \"swing\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nosLrJ08I6VK"
      },
      "source": [
        "#### Part 1: \n",
        "\n",
        "First, make a for loop that loops through all of the `unique_bids` in `scores` (you can use the variables we've defined for you). In each iteration of the loop, you should create a subset of `scores` which only contains the rows that correspond to that `bid`. Using that subset, then calculate the swing for that `bid` and append it to the array `swings`, which we have created for you. \n",
        "\n",
        "After running your code, you should have an array of numbers assigned to `swings` that represents the swing of each `bid` in the order of `unique_bids`. \n",
        "\n",
        "_Hint: when you have an array or series of data, you can use the built in python functions `max(...)` and `min(...)` to find the max and min respectively._ \n",
        "\n",
        "_Hint: you can use_ `np.append(arr, x)` _to append a number_ `x` _to an array called_ `arr`. _Remember, this makes a copy of the array, so if you want to keep using the name with the updated value, you need to reassign it._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxD2BI91I6VK"
      },
      "outputs": [],
      "source": [
        "unique_bids = scores['bid'].unique()\n",
        "swings = np.array([])\n",
        "\n",
        "## Your Code Here...\n",
        "\n",
        "for..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdLXy20-I6VK"
      },
      "outputs": [],
      "source": [
        "check('q4d1', swings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Grx8bmI6VK"
      },
      "source": [
        "In the following cell, we've made a new dataframe for you that combines `unique_bids` and `swings`, called `swings_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbRDt1MDI6VK"
      },
      "outputs": [],
      "source": [
        "# run this cell, don't modify it\n",
        "swings_df = pd.DataFrame({'bid':unique_bids, \"swing\":swings})\n",
        "swings_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf_1ZZhEI6VK"
      },
      "source": [
        "#### Part 2:\n",
        "\n",
        "Find the name of the restaurant with the largest swing and assign it to `largest_swing`.\n",
        "\n",
        "_Hint: you have to match the business ID to the restaurant name, use `bus`!_\n",
        "\n",
        "_Hint: remember the `DataFrame.sort_values` function!_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arif-TnKI6VL"
      },
      "outputs": [],
      "source": [
        "## Your Code Here...\n",
        "\n",
        "...\n",
        "\n",
        "largest_swing = ...\n",
        "largest_swing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyEIpOjcI6VL"
      },
      "outputs": [],
      "source": [
        "check('q4d3', largest_swing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2esj8zVI6VL"
      },
      "source": [
        "What a glowup! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRVONuljI6VL"
      },
      "source": [
        "# **3. Connections Between the Data**\n",
        "\n",
        "In this section we will work on trying to find some connections in the information provided by Yelp using linear regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awHvzwrtI6VL"
      },
      "source": [
        "Refer back to Lab 4 if you want a review of linear regression, but the basic intuition behind it is finding the best number to plug into _some_ equation that takes in data and outputs a prediction. \n",
        "\n",
        "We've looked at the most simple example of this, which is using a line to make a prediction with the equation $y = m * x + b$, where $y$ is our prediction and $x$ is the data that we plug into and use to predict, and $m$ and $b$ are parameters that we have to find. \n",
        "\n",
        "We've also went over using some more complex equations using quadratic and cubic variables, where our prediction equation might be $y = a*x + b * x^2 + c * x^3 + d$. In this case, $a, b, c,$ and $d$ are the parameters that we need to find the best version of \n",
        "\n",
        "As mentioned in Lecture 4, we find the best parameters by minimizing the mean-squared-error, or MSE. And we use Python to do this, since solving by hand (while doable), is time consuming and a waste of resources.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weVy8yGgI6VL"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "We'll be working with the Yelp average ratings (columns `rating`), as well as the number of review counts (column `review_count`). In addition, we'll also do some grouping by the price of each business.\n",
        "\n",
        "For ease of use, we'll assign the columns to the following variables for use throughout this section.\n",
        "\n",
        "Run this following cell to assign the variables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzUNy3xJI6VM"
      },
      "outputs": [],
      "source": [
        "# Run this cell and use these variables from here-on after\n",
        "yelp_rating = scores['rating']\n",
        "review_count = scores['review_count']\n",
        "price = scores['price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2RY0PXCI6VM"
      },
      "source": [
        "### Question 5a:\n",
        "First, let's take a look at the `review_count` variable. Make a histogram to examine the distribution of review counts in the following cell. In your call to `plot_histogram`, try including the argument `bins = x` where `x` is the number of bins to see the distribution in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9bye6MiI6VM"
      },
      "outputs": [],
      "source": [
        "## Your Code Here ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqR7NsqBI6VM"
      },
      "source": [
        "Looking at the distribution, you might notice see that it is _highly_ right skewed, (if not make sure you're plotting the right variable!)\n",
        "\n",
        "Having skewed data is problematic. Remember in Lecture 3, we mentioned how outliers will skew the mean of the distribution. A similar thing happens in linear regression, outliers will cause the predictions to be skewed as well. \n",
        "\n",
        "We will solve this by taking the natural log of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ac7hTNTI6VM"
      },
      "source": [
        "### Question 5b:\n",
        "\n",
        "Use `np.log` and take the natural log of the `review_count` data. Assigned this to the variable, `logged_review_counts`, and then plot a scatterplot of `logged_review_counts` and `yelp_rating`. Put `logged_review_counts` on the x-axis and `yelp_rating` on the y_axis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-_5VCOzI6VM"
      },
      "outputs": [],
      "source": [
        "## Your Code Here ...\n",
        "\n",
        "logged_review_counts = ...\n",
        "\n",
        "# plot scatterplot below\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmgHZOeZI6VM"
      },
      "outputs": [],
      "source": [
        "check('q5b', logged_review_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-NMqo4QI6VN"
      },
      "source": [
        "### Question 5c:\n",
        "\n",
        "Now, let's use out knowledge of Python and run linear regression to try and fit a line to this data. \n",
        "\n",
        "As before, we will be fitting a line to the data with the equation $y = mx + b$. Remember that we are using `logged_review_counts` as the input to $x$ in the equatiion, hence the $x$ in this equation represents the natural log of review counts, and the $y$ in this equation represents the yelp rating. \n",
        "\n",
        "Write the function `mse_yelp` that calculates the mean squared error for any given slope and intercept. `mse_yelp` should take in two arguments, a slope and an intercept, and calculate the mean squared error from predicting yelp review ratings from the number of reviews if we used those parameters to predict the values. \n",
        "\n",
        "_Hint: Look back to Lab 4 for the internals of this function._ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TA5nfM1I6VN"
      },
      "outputs": [],
      "source": [
        "def mse_yelp(slope, intercept):\n",
        "    \"\"\"Calculates the MSE from predicting yelp rating from logged review counts\"\"\"\n",
        "\n",
        "    ## Your Code Here...\n",
        "\n",
        "    ...\n",
        "\n",
        "    mse = ...\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqdXr45hI6VN"
      },
      "outputs": [],
      "source": [
        "check('q5c', mse_yelp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9sDZMOPI6VN"
      },
      "source": [
        "### Question 5d:\n",
        "\n",
        "Now, use the `minimize` function to solve for the best slope and intercept. Assign these to `best_slope` and `best_intercept` accordinly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwfMI9bUI6VN"
      },
      "outputs": [],
      "source": [
        "best_parameters = minimize(...)\n",
        "best_slope = ...\n",
        "best_intercept = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6euiu0DI6VN"
      },
      "outputs": [],
      "source": [
        "check('q5d', [best_slope, best_intercept])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxs4C76FI6VN"
      },
      "source": [
        "Run the following cell to plot the best slope and intercept on the scatterplot, we've provided the code for you! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F0Auf3yI6VN"
      },
      "outputs": [],
      "source": [
        "# just run this line \n",
        "plot_q5d(best_slope, best_intercept)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5bhL3J-I6VO"
      },
      "source": [
        "### Question 5e (Optional): \n",
        "\n",
        "Now, we'll do the same process, but grouping the data using the pricing of the data. We'll write a big for loop for this, looping through the price data, which we've included in the variable `prices`. \n",
        "\n",
        "Python is a dynamic language. If you change the variable assigned to a name, functions that used that name before will update and use the latest version of whatever is assigned to the name. \n",
        "\n",
        "We will exploit this and quickly run four regressions without having to rewrite code over and over again. \n",
        "\n",
        "Fill in the following code (which we've indicated in the commented sections with \"`Your Code Here...`\". Please be careful not to delete any code that is written there before, and only edit the lines indicated.\n",
        "\n",
        "Complete the following steps:\n",
        "1) Inside of the for loop, assign `data` to be a subset of `scores` that only contains restaurants with the current price. Remember, in a for loop, you can access the current value in the list you are looping through by using the first argument in the for loop, in this case `price_level`.\n",
        "\n",
        "2) Fill in the blanks of the function `mse_yelp_subset`. Instead of using the data from scores to calculate the mse, we now only want to use a subset of data that includes the specific pricelevel, which we did in step 1. Notice how we are defining a function within the for loop. In each iteration of the for loop it gets redefined, and use a new subset of the data inside of it. In essence we are making 4 `mse` functions.\n",
        "\n",
        "3) Find the best slope and intercept for the subset of data\n",
        "\n",
        "3) Access the best_slope and best_intercept and store them in the arrays, `best_slopes` and `best_intercepts`. \n",
        "\n",
        "You should end up with an array of 4 slopes and an array of 4 intercepts, each corresponding to the different price levels in `prices`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m41MyYpEI6VO"
      },
      "outputs": [],
      "source": [
        "prices = [\"$\", \"$$\", \"$$$\", \"$$$$\"]\n",
        "\n",
        "best_slopes = np.array([])\n",
        "best_intercepts = np.array([])\n",
        "\n",
        "for price in prices:\n",
        "    data = ... # Your Code Here\n",
        "\n",
        "    def mse_yelp_subset(slope, intercept):\n",
        "\n",
        "        # your code here to calculate the MSE using your subset of data\n",
        "\n",
        "        mse = ...\n",
        "        return mse\n",
        "    \n",
        "    best_vals = ... # Your Code Here...\n",
        "\n",
        "    best_slopes = np.append(...) # Your Code Here ...\n",
        "    best_intercepts = np.append(...) # Your Code Here ...'\n",
        "\n",
        "    print(\"Best Slopes\", best_slopes)\n",
        "    print(\"Best Intercepts\", best_intercepts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppGNPGOqI6VO"
      },
      "outputs": [],
      "source": [
        "check('q5e', [best_slopes, best_intercepts])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSyUB0b2I6VO"
      },
      "source": [
        "Compare the slopes of the lines of best fit within the subsets to that of regressing on the overall data. Do you notice anything interesting? \n",
        "\n",
        "You might have seen something which is called Simpson's Paradox, where when you look at subsets of a group, the correlation in the data flips in comparison to analyzing the group as a whole. As you'll see from our results, when we regressed with the full dataset, there was a negative slope. Now, in three of the subgroups, there is a positive slope instead. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQZVbGaLI6VO"
      },
      "source": [
        "# **4. Conclusion**\n",
        "\n",
        "Great job on finishing the project! Please save and run the following cell `check_all()` for credit! It's okay if the optional parts show up as False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEplVszSI6VO"
      },
      "outputs": [],
      "source": [
        "check_all()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "final_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "2f39d6a9340f9094a15ae3206b3cb83be26c4819028e15d4b820076fd30a3721"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
